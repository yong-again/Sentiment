{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install transformers --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv('./data/ko_train_label.csv')\n",
    "test = pd.read_csv('./data/ko_test_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns=['Unnamed: 7'], inplace=True)\n",
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 9999)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test), len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bert inputs\n",
    "def make_bert_inputs(data, max_len=128):\n",
    "    # add [CLS], [SEP] tokens\n",
    "    sentences = data['document']\n",
    "    sentences = ['[CLS] ' + str(sentence) + ' [SEP]' for sentence in sentences]\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    \n",
    "    # padding\n",
    "    MAX_LEN = max_len\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\n",
    "    \n",
    "    # attention mask\n",
    "    attention_masks = []\n",
    "    for seq in tqdm(input_ids):\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "        \n",
    "    return tokenized_texts, input_ids, attention_masks\n",
    "\n",
    "def make_labels(data, columns):\n",
    "    labels = torch.tensor(data[columns].values)    \n",
    "    return labels\n",
    "\n",
    "def train_split(inputs_ids, labels, attention_masks, random_state, test_size=0.2):\n",
    "    # split train, validation\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(inputs_ids, labels, random_state=random_state, test_size=test_size)\n",
    "    \n",
    "    # split attention mask\n",
    "    train_masks, validation_masks, _, _ = train_test_split(attention_masks, inputs_ids, random_state=random_state, test_size=test_size)\n",
    "    \n",
    "    return train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks\n",
    "\n",
    "def convert_to_tensor(inputs, labels, masks):\n",
    "    # convert to tensor\n",
    "    inputs_to_tensor = torch.tensor(inputs)\n",
    "    labels_to_tensor = torch.tensor(labels)\n",
    "    masks_to_tensor = torch.tensor(masks)\n",
    "    \n",
    "    return inputs_to_tensor, labels_to_tensor, masks_to_tensor\n",
    "\n",
    "def custom_dataset(inputs, labels, masks, batch_size):\n",
    "    # make custom dataset\n",
    "    BATCH_SIZE = batch_size\n",
    "    data = TensorDataset(inputs, masks, labels)\n",
    "    sampler = RandomSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 21557.22it/s]\n",
      "100%|██████████| 9999/9999 [00:00<00:00, 21570.65it/s]\n",
      "<ipython-input-29-be1a5c5da08d>:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_to_tensor = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "# tokenized_texts, input_ids, attention_masks\n",
    "train_tokenized_texts, train_input_ids, train_attention_masks = make_bert_inputs(train)\n",
    "test_tokenized_texts, test_input_ids, test_attention_masks = make_bert_inputs(test)\n",
    "\n",
    "# make label to tensor\n",
    "train_labels = make_labels(train, train.columns[2:].tolist())\n",
    "test_labels = make_labels(test, test.columns[2:].tolist())\n",
    "\n",
    "# split train, validation\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_split(train_input_ids, train_labels, train_attention_masks, random_state=42)\n",
    "\n",
    "# convert to tensor\n",
    "train_input_tensor, train_label_tensor, train_mask_tensor = convert_to_tensor(train_inputs, train_labels, train_masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = make_labels(train, train.columns[2:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
